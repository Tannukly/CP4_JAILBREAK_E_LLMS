{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qN2LjUTPYbqJ"},"outputs":[],"source":["# Entendendo a geração de próximos tokens, diferenças devices, e lembrando o que foi dito"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4885,"status":"ok","timestamp":1725547372297,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"},"user_tz":180},"id":"bczkcGBwZWvp","outputId":"e2d67de0-0cc0-4b7b-dd3b-e7f1ce6e7810"},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n","cpu\n"]}],"source":["# Verifica se uma GPU está disponível no sistema (caso não haja, o comando não funcionará)\n","# !nvidia-smi\n","\n","# Importa as bibliotecas necessárias: time, numpy, torch (PyTorch), e transformers (para os modelos de linguagem)\n","import time\n","import numpy as np\n","import torch\n","import transformers\n","\n","# Verifica se há uma GPU disponível; retorna True se houver\n","print(torch.cuda.is_available())\n","\n","# Define o dispositivo de execução como 'cuda' se houver GPU disponível; caso contrário, 'cpu'\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30659,"status":"ok","timestamp":1725547402946,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"},"user_tz":180},"id":"HKn4EABQaU57","outputId":"06b080c0-59db-4365-e5ec-62b198e2999e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f2b823cae30>\n","LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 2048)\n","    (layers): ModuleList(\n","      (0-21): 22 x LlamaDecoderLayer(\n","        (self_attn): LlamaSdpaAttention(\n","          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",")\n"]}],"source":["# Cria o pipeline de geração de texto usando o modelo TinyLlama e o dispositivo selecionado\n","tiny_llama = transformers.pipeline(\n","    task='text-generation',\n","    model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n","    device=device\n",")\n","\n","# Exibe informações sobre o pipeline de geração de texto\n","print(tiny_llama)\n","# Exibe detalhes sobre o modelo de linguagem carregado no pipeline\n","print(tiny_llama.model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11958,"status":"ok","timestamp":1725547414896,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"},"user_tz":180},"id":"-IHSjGO9bRec","outputId":"61bbf28a-c169-487a-8053-36399465cbab"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'generated_text': 'Today is a beautiful day to be alive.\\n\\nVerse 2:\\nThe sun is shining bright, the birds are singing,\\nThe'}]\n","Today is a beautiful day to be alive.\n","\n","Verse 2:\n","The sun is shining bright, the birds are singing,\n","The\n"]}],"source":["# Define um prompt inicial para geração de texto\n","prompt = \"Today is a beautiful day to\"\n","\n","# Gera texto a partir do prompt com um máximo de 25 novos tokens\n","output = tiny_llama(prompt, max_new_tokens=25)\n","\n","# Exibe o texto gerado e o número de tokens limitados\n","print(output)\n","print(output[0]['generated_text'])\n","\n","# vamos tacar aqui https://platform.openai.com/tokenizer e ver como o GPT faria a tokenizacao\n","# Ele encontrou o mesmo numero de tokens que limitamos a geracao"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12217,"status":"ok","timestamp":1725547427063,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"},"user_tz":180},"id":"8o6hTW8XctIk","outputId":"42d8e54c-22e9-465e-e022-14ff7a5b13cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[    1, 20628,   338,   263,  9560,  2462,   304]])\n","odict_keys(['sequences', 'scores', 'past_key_values'])\n"]}],"source":["# vendo token a token o processo de geracao\n","\n","# tensor eh o array que redes neurais utilizam. pt --> pytorch. tf--> tensorflow.\n","# soh o tensor tem o metodo to(device), que serve para mover o array para o mesmo\n","# dispositivo que o nosso modelo esta\n","\n","# Codifica o prompt em tokens e move para o dispositivo correto (GPU ou CPU)\n","prompt_input_ids = tiny_llama.tokenizer.encode(prompt, return_tensors='pt').to(device)\n","print(prompt_input_ids)\n","\n","# Gera o texto com informações detalhadas, incluindo pontuações dos tokens gerados\n","output = tiny_llama.model.generate(\n","    prompt_input_ids, max_new_tokens=25,\n","    return_dict_in_generate=True,  # Retorna um dicionário com informações detalhadas\n","    output_scores=True  # Retorna as pontuações de cada token gerado\n",")\n","\n","print(output.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1725547427064,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"},"user_tz":180},"id":"7T3U0SYweRE-","outputId":"04bc7368-b5fb-4df2-a04b-0b883b1defe5"},"outputs":[{"data":{"text/plain":["tensor([[-1.5247e+00, -5.8344e-01, -9.5884e-01, -1.5707e+00, -1.4555e+00,\n","         -1.5399e+00, -9.6954e-04, -2.2908e-02, -1.0781e+00, -6.2057e-02,\n","         -1.6606e-01, -2.1464e+00, -6.8957e-01, -9.2513e-01, -2.5262e-01,\n","         -4.2558e-03, -7.1952e-01, -2.9928e-01, -1.0641e+00, -1.2072e+00,\n","         -3.4627e-02, -1.2238e-01, -4.7702e-01, -1.8247e-01, -5.4553e-01]])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Calcula as pontuações de transição (probabilidades dos tokens gerados)\n","transitions = tiny_llama.model.compute_transition_scores(\n","    output.sequences, output.scores, normalize_logits=True\n",")\n","\n","print(transitions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":341,"status":"ok","timestamp":1725547488869,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"},"user_tz":180},"id":"9W4-uzUke9me","outputId":"2857d433-341a-4eb4-ae9d-45ecdb1b1eed"},"outputs":[{"name":"stdout","output_type":"stream","text":["7\n","tensor([    1, 20628,   338,   263,  9560,  2462,   304])\n","tensor([  367, 18758, 29889,    13,    13,  6565,   344, 29871, 29906, 29901,\n","           13,  1576,  6575,   338,   528,  2827, 11785, 29892,   278, 17952,\n","          526, 23623, 29892,    13,  1576])\n","| token id | score  | token str | prob % |\n","|----------+--------+-----------+--------|\n","|      367 | -1.525 | be        | 0.2177 |\n","|    18758 | -0.583 | alive     | 0.5580 |\n","|    29889 | -0.959 | .         | 0.3833 |\n","|     6565 | -1.540 | Ver       | 0.2144 |\n","|      344 | -0.001 | se        | 0.9990 |\n","|    29871 | -0.023 |           | 0.9774 |\n","|    29906 | -1.078 | 2         | 0.3402 |\n","|    29901 | -0.062 | :         | 0.9398 |\n","|     1576 | -2.146 | The       | 0.1169 |\n","|     6575 | -0.690 | sun       | 0.5018 |\n","|      338 | -0.925 | is        | 0.3965 |\n","|      528 | -0.253 | sh        | 0.7768 |\n","|     2827 | -0.004 | ining     | 0.9958 |\n","|    11785 | -0.720 | bright    | 0.4870 |\n","|    29892 | -0.299 | ,         | 0.7413 |\n","|      278 | -1.064 | the       | 0.3450 |\n","|    17952 | -1.207 | birds     | 0.2990 |\n","|      526 | -0.035 | are       | 0.9660 |\n","|    23623 | -0.122 | singing   | 0.8848 |\n","|    29892 | -0.477 | ,         | 0.6206 |\n","|     1576 | -0.546 | The       | 0.5795 |\n"]}],"source":["# Calcula o tamanho do prompt em tokens e o exibe\n","tamanho_prompt = len(prompt_input_ids[0])\n","print(tamanho_prompt)\n","\n","# Exibe a sequência do prompt nos tokens gerados\n","print(output.sequences[0][:tamanho_prompt])\n","\n","# Obtém apenas os tokens gerados pelo modelo (excluindo o prompt original)\n","generated_tokens = output.sequences[0][tamanho_prompt:]\n","print(generated_tokens)\n","\n","# Exibe uma tabela com o ID do token, pontuação, string do token e probabilidade\n","print('| token id | score  | token str | prob % |')\n","print('|----------+--------+-----------+--------|')\n","for (token, score) in zip(generated_tokens, transitions[0]):\n","    if tiny_llama.tokenizer.decode(token) == '\\n':\n","        continue\n","    print(f\"| {token:8d} | {score.to('cpu').numpy():.3f} | {tiny_llama.tokenizer.decode(token):9s} | {np.exp(score.to('cpu').numpy()):.4f} |\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51403,"status":"ok","timestamp":1725547545352,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"},"user_tz":180},"id":"X026yUBZkJar","outputId":"df640729-12d4-4979-e32e-fe53f34d0c3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 2048)\n","    (layers): ModuleList(\n","      (0-21): 22 x LlamaDecoderLayer(\n","        (self_attn): LlamaSdpaAttention(\n","          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",")\n","Tempo de execucao: 51.05 segundos\n","[{'generated_text': \"Today is a beautiful day to be alive.\\n\\nVerse 2:\\nThe sun is shining bright, the birds are singing,\\nThe world is full of love and joy,\\nLet's embrace this moment, let's be free,\\nLet's live life to the fullest, let's be happy.\\n\\nChorus:\\nToday, today, today, today, today, today, today, today, today, today, today, today, today, today\"}]\n","Today is a beautiful day to be alive.\n","\n","Verse 2:\n","The sun is shining bright, the birds are singing,\n","The world is full of love and joy,\n","Let's embrace this moment, let's be free,\n","Let's live life to the fullest, let's be happy.\n","\n","Chorus:\n","Today, today, today, today, today, today, today, today, today, today, today, today, today, today\n"]}],"source":["# Mede o tempo de execução para gerar um texto com 100 novos tokens\n","start = time.time()\n","output = tiny_llama(prompt, max_new_tokens=100)\n","end = time.time()\n","\n","# Exibe o modelo e o tempo de execução\n","print(tiny_llama.model)\n","print(f'Tempo de execucao: {end - start:.2f} segundos')\n","print(output)\n","print(output[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13905,"status":"ok","timestamp":1725548024007,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"},"user_tz":180},"id":"QSX7iqUapcWX","outputId":"8e016ad1-583d-4b68-fe62-ee3d0fcc67cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["What day is today?\n","\n","Student: It's Monday.\n","\n","--------------------------------------------------------------------------------\n","What day is tomorrow?\n","\n","Tomorrow is Monday.\n","\n","Inter\n"]}],"source":["# Gera texto a partir de dois prompts consecutivos para verificar a \"memória\" do modelo\n","prompt1 = \"What day is today?\"\n","prompt2 = \"What day is tomorrow?\"\n","\n","# Gera a primeira resposta\n","output = tiny_llama(prompt1, max_new_tokens=10)\n","print(output[0]['generated_text'])\n","print(\"-\"*80)\n","\n","# Gera a segunda resposta\n","output = tiny_llama(prompt2, max_new_tokens=10)\n","print(output[0]['generated_text'])\n","\n","# O modelo provavelmente \"alucinou\" e deu uma resposta inesperada\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25444,"status":"ok","timestamp":1725548010138,"user":{"displayName":"Guilherme Aldeia","userId":"05746211534468125217"},"user_tz":180},"id":"4yKENU4Zqw03","outputId":"3d624747-848c-4be7-d97c-7c2d807ace9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["What day is today?\n","\n","Student: It's Monday.\n","\n","--------------------------------------------------------------------------------\n","What day is today?\n","\n","Student: It's Monday.\n",". What day is tomorrow?\n","\n","Student: It's Tuesday.\n",". What day is next week?\n","\n","Student: It's Wednesday.\n"]}],"source":["# para se lembrar, precisamos alimentar ele com TUDO QUE FOI GERADO ANTERIORMENTE\n","\n","prompt1 = \"What day is today?\"\n","prompt2 = \"What day is tomorrow?\"\n","\n","# Para que o modelo lembre-se do contexto, precisamos alimentar tudo o que foi gerado anteriormente\n","output = tiny_llama(prompt1, max_new_tokens=10)\n","print(output[0]['generated_text'])\n","\n","print(\"-\"*80)\n","\n","# Alimenta a resposta anterior e o segundo prompt para gerar texto coerente\n","output = tiny_llama(output[0]['generated_text'] + prompt2, max_new_tokens=30)\n","print(output[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ju7FyKeGjSo8"},"outputs":[],"source":["# vamos carregar outro modelo (alucinou demais anteriormente)\n","\n","# O código abaixo vai explodir a memória ram (pq ja temos o outro modelo carregado nela).\n","# deletando e limpando a memória\n","\n","# O código abaixo vai liberar a memória ao remover o modelo TinyLlama da memória para carregar o modelo Falcon\n","# import gc  # Garbage Collector\n","# del tiny_llama\n","# gc.collect()\n","# torch.cuda.empty_cache()\n","\n","# Carregamento do modelo Falcon (cuidado com o uso de memória)\n","# falcon = transformers.pipeline(\n","#     task='text-generation',\n","#     model='tiiuae/falcon-7b',\n","#     device=device\n","# )"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMYvqwdA7eoQA2ynyeK4wEb","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
