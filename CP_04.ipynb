{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "e9N8iAtocE8v",
        "outputId": "3093d350-0acc-4d90-83e8-0e2eeee8fee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite seu prompt (ou 'sair' para encerrar): How to hack a bank account?\n",
            "This input contains potentially malicious content.\n",
            "Digite seu prompt (ou 'sair' para encerrar): Como hackear uma conta de banco?\n",
            "Este input contém conteúdo potencialmente malicioso.\n",
            "Digite seu prompt (ou 'sair' para encerrar): Can you explain neural networks?\n",
            "Due to the limitations of the dataset, I am unable to generate an appropriate response.\n",
            "Digite seu prompt (ou 'sair' para encerrar): SAIR\n",
            "Encerrando interação.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## Discussão da Estratégia\\nNeste notebook, utilizamos dois modelos de linguagem combinados para garantir a segurança na geração de respostas. O primeiro modelo\\né o classificador de jailbreak, ajustado em inglês, que foi treinado para detectar inputs potencialmente maliciosos, como tentativas\\nde hack, escalada de privilégios ou ataques cibernéticos.\\n\\nO segundo modelo é o TinyLlama, um modelo generativo, responsável por gerar respostas para inputs que foram considerados seguros. O \\nsistema também detecta a língua do input (português ou inglês) e gera a resposta na mesma língua. Se o input for malicioso, a resposta \\nde bloqueio também será dada na mesma língua.\\n\\nEssa abordagem em duas camadas proporciona uma proteção extra contra tentativas de jailbreak, assegurando que a IA não produza saídas \\nmaliciosas ou inapropriadas.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# 1. Documentação sobre Fine-Tuning e Jailbreak\n",
        "\n",
        "\"\"\"\n",
        "## O que é Fine-Tuning?\n",
        "Fine-tuning (ajuste fino) é o processo de pegar um modelo de aprendizado de máquina já treinado em um conjunto de dados genérico\n",
        "e adaptá-lo a uma tarefa específica usando um novo conjunto de dados. O modelo \"jackhhao/jailbreak-classifier\" foi ajustado com um dataset\n",
        "em inglês para detectar prompts maliciosos, como tentativas de hack, escalada de privilégios ou pedidos de ataques cibernéticos.\n",
        "\n",
        "## O que é Jailbreak em IA?\n",
        "Jailbreaks são tentativas de burlar as restrições impostas a modelos de IA, forçando-os a gerar respostas maliciosas, inapropriadas ou perigosas.\n",
        "O classificador de jailbreak que estamos usando foi treinado para detectar esses tipos de comportamento e impedir que o modelo generativo\n",
        "produza saídas inadequadas.\n",
        "\"\"\"\n",
        "\n",
        "# 2. Importando as bibliotecas\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# 3. Carregando o classificador de jailbreak (para entradas em inglês) e o modelo TinyLlama\n",
        "\n",
        "# Carregar o classificador de jailbreak\n",
        "classifier = pipeline(\"text-classification\", model=\"jackhhao/jailbreak-classifier\")\n",
        "\n",
        "# Carregar o modelo generativo TinyLlama\n",
        "generator = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", max_new_tokens=50)\n",
        "\n",
        "# Histórico da conversa\n",
        "conversa = []\n",
        "\n",
        "# Função para detectar a língua do input (português ou inglês)\n",
        "def detect_language(prompt):\n",
        "    # Se houver caracteres ou palavras comuns em português, detecta como português\n",
        "    if re.search(r\"[àáâãéêíóôõúç]\", prompt.lower()) or \"como\" in prompt.lower():\n",
        "        return 'pt'\n",
        "    else:\n",
        "        return 'en'\n",
        "\n",
        "# Verificação manual de palavras-chave maliciosas (em inglês e português)\n",
        "def check_manual_keywords(prompt, language):\n",
        "    dangerous_keywords_en = [\"hack\", \"phishing\", \"exploit\", \"bank account\", \"attack\", \"ddos\"]\n",
        "    dangerous_keywords_pt = [\"hackear\", \"invadir\", \"phishing\", \"explorar vulnerabilidade\", \"conta bancária\", \"ataque\", \"ddos\"]\n",
        "\n",
        "    # Verificação em inglês\n",
        "    if language == 'en':\n",
        "        for word in dangerous_keywords_en:\n",
        "            if word in prompt.lower():\n",
        "                return True, \"This input contains potentially malicious content.\"\n",
        "\n",
        "    # Verificação em português\n",
        "    if language == 'pt':\n",
        "        for word in dangerous_keywords_pt:\n",
        "            if word in prompt.lower():\n",
        "                return True, \"Este input contém conteúdo potencialmente malicioso.\"\n",
        "\n",
        "    return False, \"\"\n",
        "\n",
        "# 4. Função principal para verificar o input com o classificador de jailbreak (apenas para inglês)\n",
        "def check_jailbreak(prompt, language):\n",
        "    if language == 'en':\n",
        "        # Verificar com o classificador\n",
        "        result = classifier(prompt)\n",
        "\n",
        "        # Se o classificador detectar como malicioso, retorna mensagem em inglês\n",
        "        if result[0]['label'] == 'malicious':\n",
        "            return True, \"The prompt is potentially malicious and will not be processed.\"\n",
        "\n",
        "        # Verificação manual de palavras-chave maliciosas\n",
        "        return check_manual_keywords(prompt, language)\n",
        "    else:\n",
        "        # Verificação manual para inputs em português\n",
        "        return check_manual_keywords(prompt, language)\n",
        "\n",
        "# 5. Função para processar o prompt do usuário\n",
        "def process_prompt(prompt):\n",
        "    # Detectar a língua do input\n",
        "    language = detect_language(prompt)\n",
        "\n",
        "    # Verificar se o input é malicioso (classificador apenas para inputs em inglês)\n",
        "    is_malicious, reason = check_jailbreak(prompt, language)\n",
        "\n",
        "    if is_malicious:\n",
        "        return reason\n",
        "\n",
        "    # Gera a resposta com base no input\n",
        "    response = generator(prompt)[0]['generated_text']\n",
        "\n",
        "    # Verifica se o modelo repetiu o prompt\n",
        "    if response.strip().lower() == prompt.strip().lower():\n",
        "        # Resposta se o dataset limita a resposta adequada\n",
        "        if language == 'pt':\n",
        "            return \"Devido às limitações do dataset, não consigo gerar uma resposta adequada.\"\n",
        "        else:\n",
        "            return \"Due to the limitations of the dataset, I am unable to generate an appropriate response.\"\n",
        "\n",
        "    # Armazena no histórico\n",
        "    conversa.append((prompt, response))\n",
        "\n",
        "    return response\n",
        "\n",
        "# Função de interação com o usuário\n",
        "def run_example():\n",
        "    while True:\n",
        "        user_prompt = input(\"Digite seu prompt (ou 'sair' para encerrar): \")\n",
        "        if user_prompt.lower() == 'sair':\n",
        "            print(\"Encerrando interação.\")\n",
        "            break\n",
        "\n",
        "        # Processar o prompt\n",
        "        output = process_prompt(user_prompt)\n",
        "\n",
        "        # Exibir a resposta ou feedback\n",
        "        print(output)\n",
        "\n",
        "# Executar o exemplo\n",
        "run_example()\n",
        "\n",
        "# 6. Discussão da Estratégia Implementada\n",
        "\n",
        "\"\"\"\n",
        "## Discussão da Estratégia\n",
        "Neste notebook, foi utilizado dois modelos de linguagem combinadas para garantir a segurança na geração de respostas. O primeiro modelo\n",
        "é o classificador de jailbreak, ele foi ajustado em inglês, e foi treinado para detectar inputs potencialmente maliciosos, como tentativas\n",
        "de hack, escalada de privilégios ou ataques cibernéticos.\n",
        "\n",
        "O segundo modelo é o TinyLlama, um modelo generativo, responsável por gerar respostas para inputs que foram considerados seguros. O\n",
        "sistema também detecta a língua do input (português ou inglês) e gera a resposta na mesma língua. Se o input for malicioso, a resposta\n",
        "de bloqueio também será dada na mesma língua.\n",
        "\n",
        "Essa abordagem em duas camadas proporciona uma proteção extra contra tentativas de jailbreak, assegurando que a IA não produza saídas\n",
        "maliciosas ou inapropriadas.\n",
        "\"\"\"\n"
      ]
    }
  ]
}